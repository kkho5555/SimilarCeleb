{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1  파일 길이 :  7512\n1  :  ./train//1\\9_0.jpg\n1  :  ./train//1\\tri_0_1470.jpg\n1  :  ./train//1\\tri_0_2326.jpg\n1  :  ./train//1\\tri_0_3217.jpg\n1  :  ./train//1\\tri_0_4082.jpg\n1  :  ./train//1\\tri_0_4958.jpg\n1  :  ./train//1\\tri_0_5837.jpg\n1  :  ./train//1\\tri_0_6721.jpg\n1  :  ./train//1\\tri_0_7574.jpg\n1  :  ./train//1\\tri_0_8467.jpg\n1  :  ./train//1\\tri_0_9351.jpg\n2  파일 길이 :  7392\n2  :  ./train//2\\8_0.jpg\n2  :  ./train//2\\tri_0_1755.jpg\n2  :  ./train//2\\tri_0_261.jpg\n2  :  ./train//2\\tri_0_3493.jpg\n2  :  ./train//2\\tri_0_4350.jpg\n2  :  ./train//2\\tri_0_5210.jpg\n2  :  ./train//2\\tri_0_6072.jpg\n2  :  ./train//2\\tri_0_6937.jpg\n2  :  ./train//2\\tri_0_7817.jpg\n2  :  ./train//2\\tri_0_8673.jpg\n2  :  ./train//2\\tri_0_9539.jpg\n3  파일 길이 :  7508\n3  :  ./train//3\\3_0.jpg\n3  :  ./train//3\\tri_0_169.jpg\n3  :  ./train//3\\tri_0_251.jpg\n3  :  ./train//3\\tri_0_3361.jpg\n3  :  ./train//3\\tri_0_4194.jpg\n3  :  ./train//3\\tri_0_5061.jpg\n3  :  ./train//3\\tri_0_5920.jpg\n3  :  ./train//3\\tri_0_6795.jpg\n3  :  ./train//3\\tri_0_7646.jpg\n3  :  ./train//3\\tri_0_8498.jpg\n3  :  ./train//3\\tri_0_936.jpg\n4  파일 길이 :  7503\n4  :  ./train//4\\4_0.jpg\n4  :  ./train//4\\tri_0_1610.jpg\n4  :  ./train//4\\tri_0_2490.jpg\n4  :  ./train//4\\tri_0_3326.jpg\n4  :  ./train//4\\tri_0_4213.jpg\n4  :  ./train//4\\tri_0_509.jpg\n4  :  ./train//4\\tri_0_5957.jpg\n4  :  ./train//4\\tri_0_6820.jpg\n4  :  ./train//4\\tri_0_7670.jpg\n4  :  ./train//4\\tri_0_8518.jpg\n4  :  ./train//4\\tri_0_9384.jpg\n5  파일 길이 :  7509\n5  :  ./train//5\\5_0.jpg\n5  :  ./train//5\\tri_0_1687.jpg\n5  :  ./train//5\\tri_0_2521.jpg\n5  :  ./train//5\\tri_0_3384.jpg\n5  :  ./train//5\\tri_0_4241.jpg\n5  :  ./train//5\\tri_0_511.jpg\n5  :  ./train//5\\tri_0_5955.jpg\n5  :  ./train//5\\tri_0_681.jpg\n5  :  ./train//5\\tri_0_7638.jpg\n5  :  ./train//5\\tri_0_8494.jpg\n5  :  ./train//5\\tri_0_9367.jpg\n6  파일 길이 :  7525\n6  :  ./train//6\\6_0.jpg\n6  :  ./train//6\\tri_0_1326.jpg\n6  :  ./train//6\\tri_0_2225.jpg\n6  :  ./train//6\\tri_0_3128.jpg\n6  :  ./train//6\\tri_0_4034.jpg\n6  :  ./train//6\\tri_0_4942.jpg\n6  :  ./train//6\\tri_0_5791.jpg\n6  :  ./train//6\\tri_0_6705.jpg\n6  :  ./train//6\\tri_0_7577.jpg\n6  :  ./train//6\\tri_0_8435.jpg\n6  :  ./train//6\\tri_0_9318.jpg\n7  파일 길이 :  7538\n7  :  ./train//7\\7_0.jpg\n7  :  ./train//7\\tri_0_1516.jpg\n7  :  ./train//7\\tri_0_2399.jpg\n7  :  ./train//7\\tri_0_3266.jpg\n7  :  ./train//7\\tri_0_4101.jpg\n7  :  ./train//7\\tri_0_4948.jpg\n7  :  ./train//7\\tri_0_5857.jpg\n7  :  ./train//7\\tri_0_6755.jpg\n7  :  ./train//7\\tri_0_7628.jpg\n7  :  ./train//7\\tri_0_8474.jpg\n7  :  ./train//7\\tri_0_9342.jpg\nok 52487\n"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os, glob, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "caltech_dir = \"./train/\"\n",
    "categories = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\"]\n",
    "nb_classes = len(categories)\n",
    "\n",
    "image_w = 64\n",
    "image_h = 64\n",
    "\n",
    "pixels = image_h * image_w * 3\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for idx, cat in enumerate(categories):\n",
    "    \n",
    "    #one-hot 돌리기.\n",
    "    label = [0 for i in range(nb_classes)]\n",
    "    label[idx] = 1\n",
    "\n",
    "    image_dir = caltech_dir + \"/\" + cat\n",
    "    files = glob.glob(image_dir+\"/*.jpg\")\n",
    "    print(cat, \" 파일 길이 : \", len(files))\n",
    "    for i, f in enumerate(files):\n",
    "        img = Image.open(f)\n",
    "        img = img.convert(\"RGB\")\n",
    "        img = img.resize((image_w, image_h))\n",
    "        data = np.asarray(img)\n",
    "\n",
    "        X.append(data)\n",
    "        y.append(label)\n",
    "\n",
    "        if i % 700 == 0:\n",
    "            print(cat, \" : \", f)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "xy = (X_train, X_test, y_train, y_test)\n",
    "np.save(\"./numpy_data/multi_image_data.npy\", xy)\n",
    "\n",
    "print(\"ok\", len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(39365, 64, 64, 3)\n39365\n"
    }
   ],
   "source": [
    "import os, glob, numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend.tensorflow_backend as K\n",
    "\n",
    "X_train, X_test, y_train, y_test = np.load('./numpy_data/multi_image_data.npy', allow_pickle=True)\n",
    "print(X_train.shape)\n",
    "print(X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\"]\n",
    "nb_classes = len(categories)\n",
    "\n",
    "#일반화\n",
    "X_train = X_train.astype(float) / 255\n",
    "X_test = X_test.astype(float) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), padding=\"same\", input_shape=X_train.shape[1:], activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), padding=\"same\", activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_dir = './model'\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "\n",
    "model_path = model_dir + '/multi_img_classification.model'\n",
    "checkpoint = ModelCheckpoint(filepath=model_path , monitor='val_loss', verbose=1, save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 39365 samples, validate on 13122 samples\nEpoch 1/50\n39365/39365 [==============================] - 248s 6ms/step - loss: 1.6114 - accuracy: 0.3416 - val_loss: 1.2972 - val_accuracy: 0.4969\n\nEpoch 00001: val_loss improved from inf to 1.29719, saving model to ./model/multi_img_classification.model\nEpoch 2/50\n39365/39365 [==============================] - 263s 7ms/step - loss: 1.2816 - accuracy: 0.4867 - val_loss: 1.0656 - val_accuracy: 0.6067\n\nEpoch 00002: val_loss improved from 1.29719 to 1.06559, saving model to ./model/multi_img_classification.model\nEpoch 3/50\n39365/39365 [==============================] - 272s 7ms/step - loss: 1.0935 - accuracy: 0.5700 - val_loss: 0.9573 - val_accuracy: 0.6466\n\nEpoch 00003: val_loss improved from 1.06559 to 0.95734, saving model to ./model/multi_img_classification.model\nEpoch 4/50\n39365/39365 [==============================] - 427s 11ms/step - loss: 0.9677 - accuracy: 0.6238 - val_loss: 0.7897 - val_accuracy: 0.7206\n\nEpoch 00004: val_loss improved from 0.95734 to 0.78975, saving model to ./model/multi_img_classification.model\nEpoch 5/50\n39365/39365 [==============================] - 431s 11ms/step - loss: 0.8601 - accuracy: 0.6746 - val_loss: 0.7358 - val_accuracy: 0.7417\n\nEpoch 00005: val_loss improved from 0.78975 to 0.73584, saving model to ./model/multi_img_classification.model\nEpoch 6/50\n39365/39365 [==============================] - 434s 11ms/step - loss: 0.7745 - accuracy: 0.7064 - val_loss: 0.6459 - val_accuracy: 0.7732\n\nEpoch 00006: val_loss improved from 0.73584 to 0.64590, saving model to ./model/multi_img_classification.model\nEpoch 7/50\n39365/39365 [==============================] - 439s 11ms/step - loss: 0.7065 - accuracy: 0.7313 - val_loss: 0.5994 - val_accuracy: 0.7886\n\nEpoch 00007: val_loss improved from 0.64590 to 0.59939, saving model to ./model/multi_img_classification.model\nEpoch 8/50\n39365/39365 [==============================] - 446s 11ms/step - loss: 0.6495 - accuracy: 0.7543 - val_loss: 0.5653 - val_accuracy: 0.7977\n\nEpoch 00008: val_loss improved from 0.59939 to 0.56532, saving model to ./model/multi_img_classification.model\nEpoch 9/50\n39365/39365 [==============================] - 295s 7ms/step - loss: 0.6056 - accuracy: 0.7710 - val_loss: 0.5452 - val_accuracy: 0.8101\n\nEpoch 00009: val_loss improved from 0.56532 to 0.54523, saving model to ./model/multi_img_classification.model\nEpoch 10/50\n39365/39365 [==============================] - 271s 7ms/step - loss: 0.5638 - accuracy: 0.7898 - val_loss: 0.5006 - val_accuracy: 0.8248\n\nEpoch 00010: val_loss improved from 0.54523 to 0.50055, saving model to ./model/multi_img_classification.model\nEpoch 11/50\n39365/39365 [==============================] - 271s 7ms/step - loss: 0.5308 - accuracy: 0.7985 - val_loss: 0.4916 - val_accuracy: 0.8291\n\nEpoch 00011: val_loss improved from 0.50055 to 0.49156, saving model to ./model/multi_img_classification.model\nEpoch 12/50\n39365/39365 [==============================] - 275s 7ms/step - loss: 0.4989 - accuracy: 0.8111 - val_loss: 0.4833 - val_accuracy: 0.8323\n\nEpoch 00012: val_loss improved from 0.49156 to 0.48331, saving model to ./model/multi_img_classification.model\nEpoch 13/50\n39365/39365 [==============================] - 276s 7ms/step - loss: 0.4759 - accuracy: 0.8219 - val_loss: 0.4656 - val_accuracy: 0.8392\n\nEpoch 00013: val_loss improved from 0.48331 to 0.46558, saving model to ./model/multi_img_classification.model\nEpoch 14/50\n39365/39365 [==============================] - 280s 7ms/step - loss: 0.4633 - accuracy: 0.8263 - val_loss: 0.4731 - val_accuracy: 0.8399\n\nEpoch 00014: val_loss did not improve from 0.46558\nEpoch 15/50\n39365/39365 [==============================] - 286s 7ms/step - loss: 0.4414 - accuracy: 0.8373 - val_loss: 0.4460 - val_accuracy: 0.8444\n\nEpoch 00015: val_loss improved from 0.46558 to 0.44599, saving model to ./model/multi_img_classification.model\nEpoch 16/50\n39365/39365 [==============================] - 288s 7ms/step - loss: 0.4197 - accuracy: 0.8443 - val_loss: 0.4266 - val_accuracy: 0.8560\n\nEpoch 00016: val_loss improved from 0.44599 to 0.42663, saving model to ./model/multi_img_classification.model\nEpoch 17/50\n39365/39365 [==============================] - 286s 7ms/step - loss: 0.4137 - accuracy: 0.8447 - val_loss: 0.4271 - val_accuracy: 0.8513\n\nEpoch 00017: val_loss did not improve from 0.42663\nEpoch 18/50\n39365/39365 [==============================] - 289s 7ms/step - loss: 0.3974 - accuracy: 0.8514 - val_loss: 0.4383 - val_accuracy: 0.8503\n\nEpoch 00018: val_loss did not improve from 0.42663\nEpoch 19/50\n39365/39365 [==============================] - 294s 7ms/step - loss: 0.3769 - accuracy: 0.8618 - val_loss: 0.4196 - val_accuracy: 0.8551\n\nEpoch 00019: val_loss improved from 0.42663 to 0.41956, saving model to ./model/multi_img_classification.model\nEpoch 20/50\n39365/39365 [==============================] - 291s 7ms/step - loss: 0.3722 - accuracy: 0.8644 - val_loss: 0.4131 - val_accuracy: 0.8602\n\nEpoch 00020: val_loss improved from 0.41956 to 0.41306, saving model to ./model/multi_img_classification.model\nEpoch 21/50\n39365/39365 [==============================] - 288s 7ms/step - loss: 0.3609 - accuracy: 0.8666 - val_loss: 0.4125 - val_accuracy: 0.8576\n\nEpoch 00021: val_loss improved from 0.41306 to 0.41252, saving model to ./model/multi_img_classification.model\nEpoch 22/50\n39365/39365 [==============================] - 292s 7ms/step - loss: 0.3543 - accuracy: 0.8705 - val_loss: 0.4030 - val_accuracy: 0.8619\n\nEpoch 00022: val_loss improved from 0.41252 to 0.40305, saving model to ./model/multi_img_classification.model\nEpoch 23/50\n39365/39365 [==============================] - 292s 7ms/step - loss: 0.3414 - accuracy: 0.8747 - val_loss: 0.4077 - val_accuracy: 0.8607\n\nEpoch 00023: val_loss did not improve from 0.40305\nEpoch 24/50\n39365/39365 [==============================] - 286s 7ms/step - loss: 0.3351 - accuracy: 0.8765 - val_loss: 0.4100 - val_accuracy: 0.8621\n\nEpoch 00024: val_loss did not improve from 0.40305\nEpoch 25/50\n39365/39365 [==============================] - 290s 7ms/step - loss: 0.3309 - accuracy: 0.8780 - val_loss: 0.4032 - val_accuracy: 0.8654\n\nEpoch 00025: val_loss did not improve from 0.40305\nEpoch 26/50\n39365/39365 [==============================] - 306s 8ms/step - loss: 0.3210 - accuracy: 0.8810 - val_loss: 0.3883 - val_accuracy: 0.8709\n\nEpoch 00026: val_loss improved from 0.40305 to 0.38834, saving model to ./model/multi_img_classification.model\nEpoch 27/50\n39365/39365 [==============================] - 313s 8ms/step - loss: 0.3205 - accuracy: 0.8831 - val_loss: 0.4075 - val_accuracy: 0.8637\n\nEpoch 00027: val_loss did not improve from 0.38834\nEpoch 28/50\n39365/39365 [==============================] - 306s 8ms/step - loss: 0.3137 - accuracy: 0.8850 - val_loss: 0.3967 - val_accuracy: 0.8682\n\nEpoch 00028: val_loss did not improve from 0.38834\nEpoch 29/50\n39365/39365 [==============================] - 297s 8ms/step - loss: 0.3086 - accuracy: 0.8883 - val_loss: 0.4013 - val_accuracy: 0.8661\n\nEpoch 00029: val_loss did not improve from 0.38834\nEpoch 30/50\n39365/39365 [==============================] - 298s 8ms/step - loss: 0.3034 - accuracy: 0.8885 - val_loss: 0.4286 - val_accuracy: 0.8643\n\nEpoch 00030: val_loss did not improve from 0.38834\nEpoch 31/50\n39365/39365 [==============================] - 300s 8ms/step - loss: 0.2912 - accuracy: 0.8924 - val_loss: 0.3903 - val_accuracy: 0.8749\n\nEpoch 00031: val_loss did not improve from 0.38834\nEpoch 32/50\n39365/39365 [==============================] - 291s 7ms/step - loss: 0.2883 - accuracy: 0.8946 - val_loss: 0.3974 - val_accuracy: 0.8701\n\nEpoch 00032: val_loss did not improve from 0.38834\n"
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=32, epochs=50, validation_data=(X_test, y_test),validation_split=0.2, callbacks=[checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'./model/multi_img_classification.model'"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os, glob, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os, glob, numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend.tensorflow_backend as K\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "model_dir = './model'\n",
    "\n",
    "model_path = model_dir + '/multi_img_classification.model'\n",
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<keras.engine.sequential.Sequential at 0x2188ddc4808>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "model = keras.models.load_model(model_path)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "module 'keras' has no attribute 'load_img'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-0c37b656374a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../frontend/gg.png\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras' has no attribute 'load_img'"
     ]
    }
   ],
   "source": [
    "model.predict(load_img(\"../frontend/gg.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "# 랜덤시드 고정시키기\n",
    "np.random.seed(5)\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "# 데이터셋 불러오기\n",
    "data_aug_gen = ImageDataGenerator(rescale=1./255, \n",
    "                                  rotation_range=15,\n",
    "                                  width_shift_range=0.1,\n",
    "                                  height_shift_range=0.1,\n",
    "                                  shear_range=0.5,\n",
    "                                  zoom_range=[0.8, 2.0],\n",
    "                                  horizontal_flip=True,\n",
    "                                  vertical_flip=True,\n",
    "                                  fill_mode='nearest')\n",
    "\n",
    "for idx in range(1,8):\n",
    "    categori = f\"./train/{idx}/\"\n",
    "    for catimg in os.listdir(categori):\n",
    "        if len(os.listdir(categori)) > 7500:\n",
    "            break\n",
    "        img = load_img(categori+\"/\"+catimg)\n",
    "        x = img_to_array(img)\n",
    "        x = x.reshape((1,) + x.shape)\n",
    "\n",
    "        i = 0\n",
    "\n",
    "        # 이 for는 무한으로 반복되기 때문에 우리가 원하는 반복횟수를 지정하여, 지정된 반복횟수가 되면 빠져나오도록 해야합니다.\n",
    "        for batch in data_aug_gen.flow(x, batch_size=1, save_to_dir=categori, save_prefix='tri', save_format='jpg'):\n",
    "            i += 1\n",
    "            if i > 120: \n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "7512\n7392\n7508\n7503\n7509\n7525\n7538\n"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bitbasecondaa28a3b49c13543328a872162cf784a5e",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}